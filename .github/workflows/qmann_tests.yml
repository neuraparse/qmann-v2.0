name: QMANN Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=src/qmann --cov-report=xml -m "not hardware and not slow"
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  simulator-benchmarks:
    name: Simulator Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[benchmark]"

    - name: Run simulator benchmarks
      run: |
        pytest tests/benchmarks/ -v -m benchmark
      continue-on-error: true

  error-mitigation-tests:
    name: Error Mitigation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run error mitigation tests
      run: |
        pytest tests/error_mitigation/ -v
      continue-on-error: true

  application-tests:
    name: Application Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run application tests
      run: |
        pytest tests/test_*applications.py -v
      continue-on-error: true

  ablation-studies:
    name: Ablation Studies
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run ablation studies
      run: |
        pytest tests/ablation/ -v
      continue-on-error: true

  continual-learning-tests:
    name: Continual Learning Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run continual learning tests
      run: |
        pytest tests/continual/ -v
      continue-on-error: true

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
      continue-on-error: true

  hardware-tests:
    name: Hardware Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[quantum]"
    
    - name: Run hardware validation
      env:
        IBM_QUANTUM_TOKEN: ${{ secrets.IBM_QUANTUM_TOKEN }}
      run: |
        pytest tests/ -v -m hardware --slow
      continue-on-error: true

  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Lint with flake8
      run: |
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
      continue-on-error: true

    - name: Format check with black
      run: |
        black --check --line-length=120 src/ tests/ || true

    - name: Type check with mypy
      run: |
        mypy src/qmann --ignore-missing-imports || true

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run security scan with bandit
      run: |
        bandit -r src/ -f json -o bandit-report.json || true

    - name: Check dependencies with safety
      run: |
        safety check --json || true

  test-report:
    name: Test Report
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [unit-tests, simulator-benchmarks, error-mitigation-tests,
            application-tests, ablation-studies, continual-learning-tests,
            integration-tests]
    if: always()

    steps:
    - name: Check test results
      run: |
        if [ "${{ needs.unit-tests.result }}" != "success" ]; then
          echo "Unit tests failed"
          exit 1
        fi
        if [ "${{ needs.simulator-benchmarks.result }}" != "success" ]; then
          echo "Simulator benchmarks failed"
          exit 1
        fi
        echo "All tests passed!"

